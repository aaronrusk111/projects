{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60023,
     "status": "ok",
     "timestamp": 1726709477938,
     "user": {
      "displayName": "Aaron Rusk",
      "userId": "14574469382624111100"
     },
     "user_tz": 240
    },
    "id": "3W1C8yec5pG3",
    "outputId": "ca210c7b-2321-4315-bbd7-1320ee77cd35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849768 sha256=c7baad0585caeff2db3700178a14aa4d26a34bb46c06e51e0c357ee26869fc80\n",
      "  Stored in directory: /Users/aaronrusk/Library/Caches/pip/wheels/8d/28/22/5dbae8a8714ef046cebd320d0ef7c92f5383903cf854c15c0c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7402,
     "status": "ok",
     "timestamp": 1726712487160,
     "user": {
      "displayName": "Aaron Rusk",
      "userId": "14574469382624111100"
     },
     "user_tz": 240
    },
    "id": "E40wuEoW7l0V",
    "outputId": "252baa4c-e469-409c-a66e-00c492705cba"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      4\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\n\u001b[1;32m      5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from google.colab import drive\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "lines = sc.textFile(\"/content/drive/My Drive/Datasets/fradulent_emails.txt\")\n",
    "\n",
    "# Splits lines into words and maps each word to (word, 1)\n",
    "def processLine(line):\n",
    "    words = line.split(\" \")\n",
    "    return [(word.lower(), 1) for word in words]\n",
    "\n",
    "# Sum the counts for each word ie reduce by key\n",
    "def sumWordCounts(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Sort word counts in descending order to easily get top 20\n",
    "def sortDesc(pair):\n",
    "    return -pair[1]\n",
    "\n",
    "myWords = lines.flatMap(processLine)\n",
    "wordCounts = myWords.reduceByKey(sumWordCounts)\n",
    "top20Words = wordCounts.takeOrdered(21, key=sortDesc)\n",
    "# Got top 21 words because the most used \"word\" is : , which is not actually a word\n",
    "\n",
    "for word, count in top20Words:\n",
    "    print(str(word) + \" : \" + str(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1726712477064,
     "user": {
      "displayName": "Aaron Rusk",
      "userId": "14574469382624111100"
     },
     "user_tz": 240
    },
    "id": "w9edlUhZ9d-4"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeutQcplFxdD"
   },
   "source": [
    "The most frequently used words in order are:\n",
    "the, to, of, and, i, in, you, this, a, my, your, for, will, that, as, is, be, with, from, me\n",
    "\n",
    "This count does not really give any indication that they are taken from phishing emails, they are simply 20 of what I would predict are some of the most commonly used words in the English language. To improve the word count tool, we could purposely omit these super common conjuctions, pronouns, etc. that would be common in any English text. We could have a list of these common words and compare the words we find in the emails, omitting the words that also appear in that list. Then what would be left are the most used uncommon words, which would be a better indicator of if an email is likely to be a scam."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOMn0x70n1tVq1l7q1YmC7N",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
