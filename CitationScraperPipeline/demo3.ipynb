{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from readability import Document\n",
    "from newspaper import Article\n",
    "from bs4 import BeautifulSoup\n",
    "from serpapi import GoogleSearch\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 URLs.\n"
     ]
    }
   ],
   "source": [
    "api_key = \"c9691012c0d88d2800f8b26247609a0231402853f0c2cb6c89c3d9d1fa39a41f\"  # Replace with your SerpAPI key\n",
    "article_citation_id = \"1NtVbf1efHoJ\"  # Replace with your citation ID\n",
    "\n",
    "# List to store URLs\n",
    "urls = []\n",
    "# Paginate through results, stopping after retrieving 100 URLs\n",
    "start = 0  # Pagination index\n",
    "MAX_RESULTS = 100\n",
    "\n",
    "while len(urls) < MAX_RESULTS:\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"cites\": article_citation_id,\n",
    "        \"api_key\": api_key,\n",
    "        \"start\": start\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results.get(\"organic_results\", [])\n",
    "\n",
    "    if not organic_results:\n",
    "        break  # No more results, exit the loop\n",
    "\n",
    "    for result in organic_results:\n",
    "        urls.append(result[\"link\"])\n",
    "        if len(urls) >= MAX_RESULTS:\n",
    "            break  # Stop once we reach the desired number of articles\n",
    "\n",
    "    start += 10  # Move to the next page\n",
    "print(f\"Retrieved {len(urls)} URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing content extraction libraries...\n",
      "\n",
      "Success Rates (Percentage of URLs with content extracted):\n",
      "Readability      49.0\n",
      "Newspaper3k      55.0\n",
      "BeautifulSoup    50.0\n",
      "dtype: float64\n",
      "\n",
      "Number of URLs failed by all extractors: 45\n",
      "URLs failed by all extractors:\n",
      "0     https://www.cell.com/cell-systems/fulltext/S24...\n",
      "1     https://ascopubs.org/doi/abs/10.1200/CCI.18.00069\n",
      "2     https://www.cell.com/trends/cell-biology/fullt...\n",
      "4     https://www.tandfonline.com/doi/abs/10.1080/19...\n",
      "6     https://www.cell.com/trends/cancer/fulltext/S2...\n",
      "11    https://www.embopress.org/doi/abs/10.15252/msb...\n",
      "12    https://pubs.acs.org/doi/abs/10.1021/acs.chemr...\n",
      "15    https://onlinelibrary.wiley.com/doi/abs/10.100...\n",
      "16    https://ieeexplore.ieee.org/abstract/document/...\n",
      "17    https://academic.oup.com/bioinformatics/articl...\n",
      "19    https://www.sciencedirect.com/science/article/...\n",
      "21    https://royalsocietypublishing.org/doi/abs/10....\n",
      "22    https://wires.onlinelibrary.wiley.com/doi/abs/...\n",
      "29    https://wires.onlinelibrary.wiley.com/doi/abs/...\n",
      "30    https://academic.oup.com/bioinformatics/articl...\n",
      "33    https://www.cell.com/iscience/fulltext/S2589-0...\n",
      "34    https://www.sciencedirect.com/science/article/...\n",
      "35    https://academic.oup.com/bioinformatics/articl...\n",
      "36    https://www.sciencedirect.com/science/article/...\n",
      "37    https://royalsocietypublishing.org/doi/abs/10....\n",
      "40    https://academic.oup.com/bib/article-abstract/...\n",
      "46    https://academic.oup.com/bioinformatics/articl...\n",
      "49    https://onlinelibrary.wiley.com/doi/abs/10.100...\n",
      "55    https://www.sciencedirect.com/science/article/...\n",
      "56    https://onlinelibrary.wiley.com/doi/abs/10.100...\n",
      "58    https://www.sciencedirect.com/science/article/...\n",
      "60    https://www.sciencedirect.com/science/article/...\n",
      "67    https://www.cell.com/iscience/fulltext/S2589-0...\n",
      "69    https://academic.oup.com/bioinformatics/articl...\n",
      "70    https://www.jbc.org/article/S0021-9258(17)4859...\n",
      "72    https://www.cell.com/med/fulltext/S2666-6340(2...\n",
      "76    https://academic.oup.com/gigascience/article-a...\n",
      "79    https://aacrjournals.org/cancerres/article-abs...\n",
      "80    https://www.sciencedirect.com/science/article/...\n",
      "81    https://www.sciencedirect.com/science/article/...\n",
      "82    https://www.sciencedirect.com/science/article/...\n",
      "83    https://www.sciencedirect.com/science/article/...\n",
      "85    https://www.cell.com/cell-stem-cell/fulltext/S...\n",
      "88    https://onlinelibrary.wiley.com/doi/abs/10.100...\n",
      "89    https://royalsocietypublishing.org/doi/abs/10....\n",
      "93    https://academic.oup.com/gigascience/article-p...\n",
      "95    https://www.sciencedirect.com/science/article/...\n",
      "96    https://www.sciencedirect.com/science/article/...\n",
      "97    https://www.sciencedirect.com/science/article/...\n",
      "98    https://royalsocietypublishing.org/doi/abs/10....\n",
      "Name: URL, dtype: object\n",
      "\n",
      "Results saved to 'content_extraction_comparison.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Function to extract content using Readability\n",
    "def extract_readability(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        doc = Document(response.text)\n",
    "        content = doc.summary()\n",
    "        return content\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Function to extract content using Newspaper3k\n",
    "def extract_newspaper(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Function to extract content using BeautifulSoup\n",
    "def extract_beautifulsoup(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "        content = \"\\n\".join(paragraphs)\n",
    "        return content if content.strip() else None\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Compare all extractors on a single URL\n",
    "def process_url(url):\n",
    "    result = {\"URL\": url}\n",
    "\n",
    "    # Extract content using all libraries\n",
    "    result[\"Readability\"] = bool(extract_readability(url))\n",
    "    result[\"Newspaper3k\"] = bool(extract_newspaper(url))\n",
    "    result[\"BeautifulSoup\"] = bool(extract_beautifulsoup(url))\n",
    "\n",
    "    return result\n",
    "\n",
    "# Run comparison on all URLs\n",
    "def compare_extractors(urls):\n",
    "    results = []\n",
    "\n",
    "    # Use multithreading to process URLs in parallel\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(executor.map(process_url, urls))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Main script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Comparing content extraction libraries...\")\n",
    "    comparison_results = compare_extractors(urls)\n",
    "\n",
    "    # Convert results to a DataFrame for analysis\n",
    "    df = pd.DataFrame(comparison_results)\n",
    "\n",
    "    # Determine overall success rates\n",
    "    success_rates = df.drop(columns=[\"URL\"]).mean() * 100\n",
    "\n",
    "    print(\"\\nSuccess Rates (Percentage of URLs with content extracted):\")\n",
    "    print(success_rates)\n",
    "\n",
    "    # Find common URLs that failed across all extractors\n",
    "    failed_all = df[\n",
    "        (df[\"Readability\"] == False) &\n",
    "        (df[\"Newspaper3k\"] == False) &\n",
    "        (df[\"BeautifulSoup\"] == False)\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nNumber of URLs failed by all extractors: {len(failed_all)}\")\n",
    "    print(\"URLs failed by all extractors:\")\n",
    "    print(failed_all[\"URL\"])\n",
    "\n",
    "    # Save results to a CSV file for further analysis\n",
    "    df.to_csv(\"content_extraction_comparison.csv\", index=False)\n",
    "    print(\"\\nResults saved to 'content_extraction_comparison.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
